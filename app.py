# -*- coding: utf-8 -*-
"""YOLO-World Prompt Detector

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BMFEu2FoZhuigVKuj7KwC34WIkw4ED5f
"""

!pip install ultralytics
!pip install ftfy regex tqdm
!pip install git+https://github.com/openai/CLIP.git
!pip install opencv-python pillow matplotlib

!pip install gradio ultralytics git+https://github.com/openai/CLIP.git ftfy regex tqdm

import torch
import clip
from PIL import Image
import cv2
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO
import gradio as gr
import torch

# Step 1: Install requirements
!pip install gradio ultralytics git+https://github.com/openai/CLIP.git ftfy regex tqdm --quiet

# Step 2: Import libraries
import gradio as gr
import torch
import clip
from PIL import Image
from ultralytics import YOLO
import numpy as np
import cv2

# Step 3: Load models
device = "cuda" if torch.cuda.is_available() else "cpu"
clip_model, preprocess = clip.load("ViT-B/32", device=device)
yolo_model = YOLO("yolov8s.pt")  # Or use yolov8n.pt for smaller model

# Step 4: Define function
def detect_object(image, prompt):
    img = image.convert("RGB")
    results = yolo_model(img)[0]
    boxes = results.boxes
    names = yolo_model.names

    region_images = []
    region_info = []

    for box in boxes:
        cls_id = int(box.cls[0])
        label = names[cls_id]
        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
        cropped = img.crop((x1, y1, x2, y2))
        region_images.append(preprocess(cropped).unsqueeze(0))
        region_info.append((label, (x1, y1, x2, y2)))

    if region_images:
        with torch.no_grad():
            image_features = clip_model.encode_image(torch.cat(region_images).to(device))
            text_features = clip_model.encode_text(clip.tokenize([prompt]).to(device))
            similarities = torch.cosine_similarity(image_features, text_features)
            best_idx = similarities.argmax().item()

        label, (x1, y1, x2, y2) = region_info[best_idx]
        img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
        cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 3)
        cv2.putText(img_cv, prompt, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX,
                    1, (0, 255, 0), 2, cv2.LINE_AA)
        return Image.fromarray(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))
    else:
        return "No objects detected!"

# Step 5: Gradio UI
gr.Interface(
    fn=detect_object,
    inputs=[
        gr.Image(type="pil", label="Upload or Capture Image"),
        gr.Textbox(label="Enter Object Prompt (e.g. 'dog', 'laptop')")
    ],
    outputs=gr.Image(label="Detected Object"),
    title="Prompt-Based Object Detection (YOLO + CLIP)",
    description="Upload an image and enter a text prompt. The app detects the object most similar to the prompt."
).launch(share=True)

from google.colab import files
uploaded = files.upload()

# Get first uploaded image path
image_path = list(uploaded.keys())[0]
Image.open(image_path)

from IPython.display import display, Javascript
from google.colab.output import eval_js
from base64 import b64decode
import io

def take_photo(filename='webcam.jpg', quality=0.8):
    js = Javascript('''
        async function takePhoto(quality) {
            const div = document.createElement('div');
            const capture = document.createElement('button');
            capture.textContent = 'üì∑ Capture';
            div.appendChild(capture);

            const video = document.createElement('video');
            video.style.display = 'block';
            const stream = await navigator.mediaDevices.getUserMedia({video: true});
            document.body.appendChild(div);
            div.appendChild(video);
            video.srcObject = stream;
            await video.play();

            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);
            await new Promise((resolve) => capture.onclick = resolve);

            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0);
            stream.getTracks().forEach(track => track.stop());
            div.remove();

            return canvas.toDataURL('image/jpeg', quality);
        }
        takePhoto(%f);
    ''' % quality)

    display(js)
    data = eval_js("takePhoto(%f)" % quality)
    binary = b64decode(data.split(',')[1])
    with open(filename, 'wb') as f:
        f.write(binary)
    return filename

# Capture from webcam
image_path = take_photo()
Image.open(image_path)

prompt = input("Enter object to detect (e.g., 'bottle', 'dog', 'laptop'): ")
img = Image.open(image_path).convert("RGB")
results = yolo_model(image_path)[0]
boxes = results.boxes
names = yolo_model.names

region_images = []
region_info = []

for box in boxes:
    cls_id = int(box.cls[0])
    label = names[cls_id]
    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)
    cropped = img.crop((x1, y1, x2, y2))
    region_images.append(preprocess(cropped).unsqueeze(0))
    region_info.append((label, (x1, y1, x2, y2)))

if region_images:
    with torch.no_grad():
        image_features = clip_model.encode_image(torch.cat(region_images).to(device))
        text_features = clip_model.encode_text(clip.tokenize([prompt]).to(device))
        similarities = torch.cosine_similarity(image_features, text_features)
        best_idx = similarities.argmax().item()

    label, (x1, y1, x2, y2) = region_info[best_idx]
    img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)
    cv2.rectangle(img_cv, (x1, y1), (x2, y2), (0, 255, 0), 2)
    cv2.putText(img_cv, prompt, (x1, y1 - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

    # Show result
    plt.figure(figsize=(10, 6))
    plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))
    plt.axis("off")
    plt.title(f"Matched Object: {prompt}")
    plt.show()
else:
    print("‚ùå No objects detected by YOLO.")